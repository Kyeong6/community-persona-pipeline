from src.crawlers.base_crawler import BaseCrawler
from src.models.post import Post
from typing import List, Dict, Optional
import re
from datetime import datetime, timedelta


class FmkoreaCrawler(BaseCrawler):
    def __init__(self):
        super().__init__()
        self.popular_url = "https://www.fmkorea.com/index.php?mid=hotdeal&sort_index=pop&order_type=desc"
        self.channel = "fmkorea"
    
    async def crawl(self, max_posts: int = None) -> List[Post]:
        """ì—í¨ì½”ë¦¬ì•„ ì¸ê¸°ê¸€ í¬ë¡¤ë§ (ì˜¤ëŠ˜ ê¸°ì¤€ ì¼ì£¼ì¼ ì „ê¹Œì§€)"""
        posts = []
        
        try:
            # 1. ì¸ê¸°ê¸€ í˜ì´ì§€ ì ‘ì†
            print(f"ğŸ«› ì¸ê¸°ê¸€ í˜ì´ì§€ ì ‘ì†: {self.popular_url}")
            
            # ì¬ì‹œë„ ë¡œì§
            max_retries = 3
            for retry in range(max_retries):
                try:
                    response = await self.page.goto(
                        self.popular_url,
                        wait_until="load",
                        timeout=30000
                    )
                    if response:
                        print(f"ğŸ«› í˜ì´ì§€ ì‘ë‹µ ìƒíƒœ: {response.status}")
                        if response.status != 200:
                            raise Exception(f"HTTP ìƒíƒœ ì½”ë“œ ì˜¤ë¥˜: {response.status}")
                    
                    await self.page.wait_for_timeout(3000)
                    
                    # í˜ì´ì§€ ë‚´ìš© í™•ì¸
                    page_title = await self.page.title()
                    print(f"ğŸ«› í˜ì´ì§€ ì œëª©: {page_title[:50]}...")
                    
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        print(f"ğŸ«› í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retry + 1}/{max_retries}): {e}")
                        await self.page.wait_for_timeout(5000)
                    else:
                        print(f"ğŸ«› í˜ì´ì§€ ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
            
            # 2. ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ (ì¼ì£¼ì¼ ì „ê¹Œì§€ í•„í„°ë§)
            post_items = await self._get_posts_from_popular_page(max_posts)
            print(f"ğŸ«› ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ëª©ë¡: {len(post_items)}ê°œ")
            
            # 3. ê° ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            for i, item in enumerate(post_items):
                try:
                    post_url = item.get('url', '')
                    title = item.get('title', '')
                    
                    print(f"ğŸ«› ê²Œì‹œê¸€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {post_url} [{i+1}/{len(post_items)}]")
                    
                    # ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì ‘ì†
                    await self.page.goto(
                        post_url,
                        wait_until="load",
                        timeout=30000
                    )
                    await self.page.wait_for_timeout(1000)
                    
                    # ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ
                    post = await self._extract_post_data(post_url, title)
                    if post:
                        posts.append(post)
                        
                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                        await self.page.wait_for_timeout(1000)
                        
                except Exception as e:
                    print(f"ğŸ«› ê²Œì‹œê¸€ {post_url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                    
        except Exception as e:
            print(f"ğŸ«› ì—í¨ì½”ë¦¬ì•„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"ğŸ«› ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts
    
    async def _get_posts_from_popular_page(self, max_posts: int = None) -> List[Dict]:
        """ì¸ê¸°ê¸€ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ë¥¼ ìˆ˜ì§‘ (ì˜¤ëŠ˜ ê¸°ì¤€ ì¼ì£¼ì¼ ì „ê¹Œì§€)"""
        print(f"ğŸ«› ì¸ê¸°ê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        
        # ë‚ ì§œ í•„í„°: ì˜¤ëŠ˜ ê¸°ì¤€ ì¼ì£¼ì¼ ì „
        today = datetime.now()
        week_ago = today - timedelta(days=7)
        print(f"ğŸ«› ë‚ ì§œ í•„í„°: {week_ago.strftime('%Y-%m-%d')} ~ {today.strftime('%Y-%m-%d')}")
        
        collected_items: List[Dict] = []
        current_page = 1
        max_pages = 200  # ì¶©ë¶„íˆ í° ê°’
        
        while current_page <= max_pages:
            await self.page.wait_for_timeout(2000)
            
            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            items = await self.page.evaluate("""
                (() => {
                    const items = [];
                    // ê²Œì‹œê¸€ ëª©ë¡ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ ul, li ë˜ëŠ” div êµ¬ì¡°)
                    const articleSelectors = [
                        'ul.bd_lst li',
                        'li.li',
                        '.hotdeal_list li',
                        '[class*="list"] li',
                        'div[class*="article"]',
                        'tr[class*="list"]'
                    ];
                    
                    let rows = [];
                    for (const sel of articleSelectors) {
                        rows = Array.from(document.querySelectorAll(sel));
                        if (rows.length > 0) {
                            console.log('ê²Œì‹œê¸€ ëª©ë¡ ë°œê²¬:', sel, 'ê°œìˆ˜:', rows.length);
                            break;
                        }
                    }
                    
                    for (const row of rows) {
                        // ì œëª© ë§í¬ ì°¾ê¸°
                        const titleLink = row.querySelector('a[href*="/"], a[href*="index.php"]');
                        if (!titleLink) continue;
                        
                        const href = titleLink.getAttribute('href');
                        if (!href) continue;
                        
                        // URL ìƒì„±
                        let fullUrl = href;
                        if (href.startsWith('/')) {
                            fullUrl = 'https://www.fmkorea.com' + href;
                        } else if (!href.startsWith('http')) {
                            fullUrl = 'https://www.fmkorea.com/' + href;
                        }
                        
                        // ì œëª© ì¶”ì¶œ
                        const titleText = titleLink.innerText.trim() || titleLink.textContent.trim();
                        if (!titleText) continue;
                        
                        // ë‚ ì§œ ì¶”ì¶œ (span.date.m_no ë˜ëŠ” ìœ ì‚¬í•œ êµ¬ì¡°)
                        let dateText = '';
                        const dateElem = row.querySelector('span.date, .date, [class*="date"]');
                        if (dateElem) {
                            dateText = dateElem.innerText.trim();
                        }
                        
                        items.push({
                            url: fullUrl,
                            title: titleText,
                            dateText: dateText
                        });
                    }
                    
                    return items;
                })()
            """)
            
            print(f"ğŸ«› [í˜ì´ì§€ {current_page}] í™”ë©´ì—ì„œ ë°œê²¬ëœ ê²Œì‹œê¸€ ìˆ˜: {len(items)}")
            
            before_len = len(collected_items)
            found_old_posts = False
            
            for item in items:
                url = item.get('url', '')
                title = item.get('title', '')
                date_text = item.get('dateText', '')
                
                # ë‚ ì§œ í•„í„°ë§ (ì¼ì£¼ì¼ ì „ê¹Œì§€)
                if date_text:
                    post_date = self._parse_date(date_text)
                    if post_date:
                        if post_date < week_ago:
                            print(f"ğŸ«› ì œì™¸: ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ - {date_text}")
                            found_old_posts = True
                            continue
                    else:
                        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì œì™¸
                        print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, ì œì™¸: {date_text}")
                        continue
                else:
                    # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì œì™¸
                    print(f"ğŸ«› ë‚ ì§œ ì •ë³´ ì—†ìŒ, ì œì™¸")
                    continue
                
                # ì¤‘ë³µ ì²´í¬ (URL ê¸°ë°˜)
                if url and url not in [item['url'] for item in collected_items]:
                    collected_items.append(item)
            
            after_len = len(collected_items)
            new_count = after_len - before_len
            print(f"ğŸ«› [í˜ì´ì§€ {current_page}] ì‹ ê·œ ìˆ˜ì§‘: {new_count}ê°œ, ëˆ„ì : {after_len}ê°œ")
            
            # ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ë§Œ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
            if found_old_posts and new_count == 0:
                print(f"ğŸ«› ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ë§Œ ë‚¨ì•„ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            # max_posts ì œí•œì´ ìˆìœ¼ë©´ ì²´í¬
            if max_posts and len(collected_items) >= max_posts:
                print(f"ğŸ«› max_posts({max_posts})ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            if current_page < max_pages:
                next_page_clicked = False
                try:
                    next_page_num = current_page + 1
                    
                    # ë°©ë²• 1: í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸° (a[href*="page=X"])
                    next_page_button = await self.page.query_selector(f'a[href*="page={next_page_num}"]')
                    if not next_page_button:
                        # ë°©ë²• 2: "ë‹¤ìŒ" ë²„íŠ¼ ì°¾ê¸° (class="direction")
                        next_button = await self.page.query_selector('a.direction[href*="page="]')
                        if next_button:
                            # ë‹¤ìŒ ë²„íŠ¼ì˜ hrefì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                            next_href = await next_button.get_attribute('href')
                            if next_href:
                                page_match = re.search(r'page=(\d+)', next_href)
                                if page_match:
                                    next_page_num = int(page_match.group(1))
                                    next_page_button = next_button
                    
                    if next_page_button:
                        await next_page_button.click()
                        next_page_clicked = True
                        print(f"ğŸ«› í˜ì´ì§€ {next_page_num} ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                    
                    if next_page_clicked:
                        await self.page.wait_for_timeout(3000)
                        # URLì—ì„œ í˜„ì¬ í˜ì´ì§€ í™•ì¸
                        current_url = self.page.url
                        if 'page=' in current_url:
                            page_match = re.search(r'page=(\d+)', current_url)
                            if page_match:
                                current_page = int(page_match.group(1))
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (URL í™•ì¸)")
                            else:
                                current_page = next_page_num
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (ì¶”ì •)")
                        else:
                            # URLì— í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ í™œì„± í˜ì´ì§€ í™•ì¸
                            active_page = await self.page.evaluate("""
                                (() => {
                                    const activeLink = document.querySelector('strong.this, a.this, .pagination a.on');
                                    if (activeLink) {
                                        const text = activeLink.innerText.trim();
                                        const num = parseInt(text);
                                        if (!isNaN(num)) {
                                            return num;
                                        }
                                    }
                                    return null;
                                })()
                            """)
                            if active_page:
                                current_page = active_page
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (í™œì„± í˜ì´ì§€ í™•ì¸)")
                            else:
                                current_page = next_page_num
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (ì¶”ì •)")
                    else:
                        print(f"ğŸ«›âŒ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í•¨. ìˆ˜ì§‘ ì¢…ë£Œ")
                        break
                        
                except Exception as e:
                    print(f"ğŸ«›âŒ í˜ì´ì§€ë„¤ì´ì…˜ í´ë¦­ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
                    break
            else:
                print(f"ğŸ«› ìµœëŒ€ í˜ì´ì§€({max_pages})ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
        
        # max_posts ì œí•œ ì ìš©
        if max_posts:
            post_items = collected_items[:max_posts]
        else:
            post_items = collected_items
        
        print(f"ğŸ«› ì´ {len(post_items)}ê°œ ì¸ê¸°ê¸€ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {current_page}í˜ì´ì§€ ìˆœíšŒ)")
        return post_items
    
    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜"""
        if not date_text:
            return None
        
        try:
            date_text = date_text.strip()
            
            # í˜•ì‹ 1: "2025.11.04 18:25" (YYYY.MM.DD HH:MM)
            match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s+(\d{1,2}):(\d{1,2})', date_text)
            if match:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                hour = int(match.group(4))
                minute = int(match.group(5))
                return datetime(year, month, day, hour, minute)
            
            # í˜•ì‹ 2: "2025-11-04 18:25" (YYYY-MM-DD HH:MM)
            match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{1,2})', date_text)
            if match:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                hour = int(match.group(4))
                minute = int(match.group(5))
                return datetime(year, month, day, hour, minute)
            
            # í˜•ì‹ 3: "2025.11.03" (YYYY.MM.DD) - ë‚ ì§œë§Œ, ì‹œê°„ ì—†ìŒ (00:00ìœ¼ë¡œ ì„¤ì •)
            match = re.match(r'^(\d{4})\.(\d{1,2})\.(\d{1,2})$', date_text)
            if match:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return datetime(year, month, day, 0, 0)
            
            # í˜•ì‹ 4: "20:43", "20:12" (HH:MM) - ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°€ì •
            match = re.match(r'^(\d{1,2}):(\d{1,2})$', date_text)
            if match:
                today = datetime.now()
                hour = int(match.group(1))
                minute = int(match.group(2))
                return today.replace(hour=hour, minute=minute, second=0, microsecond=0)
            
            # í˜•ì‹ 5: "11.04 18:25" (MM.DD HH:MM) - ì˜¬í•´ë¡œ ê°€ì •
            match = re.search(r'(\d{1,2})\.(\d{1,2})\s+(\d{1,2}):(\d{1,2})', date_text)
            if match:
                today = datetime.now()
                month = int(match.group(1))
                day = int(match.group(2))
                hour = int(match.group(3))
                minute = int(match.group(4))
                return today.replace(month=month, day=day, hour=hour, minute=minute, second=0, microsecond=0)
            
        except Exception as e:
            print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_text} - {e}")
            return None
        
        return None
    
    async def _extract_post_data(self, post_url: str, title_from_list: str) -> Optional[Post]:
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ (h1.np_18px > span.np_18px_span)
            title = title_from_list
            title_elem = await self.page.query_selector('h1.np_18px span.np_18px_span')
            if not title_elem:
                # ëŒ€ì²´ ë°©ë²•: h1.np_18px ë˜ëŠ” span.np_18px_span
                title_elem = await self.page.query_selector('h1.np_18px, span.np_18px_span')
            if title_elem:
                title_text = await title_elem.inner_text()
                if title_text and title_text.strip():
                    title = title_text.strip()
                    # ì—¬ëŸ¬ ì¤„ì¼ ê²½ìš° ì²« ë²ˆì§¸ ì¤„ë§Œ
                    title = title.split('\n')[0].strip()
            
            # own_company: ì œëª©ì— "ë¡¯ë°ì˜¨"ì´ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0
            own_company = 1 if title and 'ë¡¯ë°ì˜¨' in title else 0
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ë§Œ, ì´ë¯¸ì§€ ì œì™¸, URL ë° íƒ­ ë¬¸ì ì œê±°)
            content = ""
            content_selectors = [
                '.rd_body',
                '.xe_content',
                '[class*="content"]',
                '[class*="body"]',
                '.document_content',
                'div[class*="article"]'
            ]
            
            for sel in content_selectors:
                try:
                    content_elem = await self.page.query_selector(sel)
                    if content_elem:
                        # ì´ë¯¸ì§€ì™€ ë§í¬ ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        content = await content_elem.evaluate("""
                            (elem) => {
                                // ì´ë¯¸ì§€ì™€ ë§í¬ ì œê±°
                                const clone = elem.cloneNode(true);
                                clone.querySelectorAll('img, a.highslide').forEach(el => el.remove());
                                return clone.innerText.trim();
                            }
                        """)
                        if content:
                            # URL ì œê±° (https://www.fmkorea.com/ìˆ«ì íŒ¨í„´)
                            content = re.sub(r'https://www\.fmkorea\.com/\d+', '', content)
                            # "ë³µì‚¬" í…ìŠ¤íŠ¸ ì œê±°
                            content = re.sub(r'ë³µì‚¬\s*', '', content)
                            # íƒ­ ë¬¸ì(\t) ì œê±°
                            content = content.replace('\t', ' ')
                            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                            content = re.sub(r'\s+', ' ', content)
                            # ì¤„ë°”ê¿ˆ ì •ë¦¬
                            lines = [line.strip() for line in content.split('\n') if line.strip()]
                            content = '\n'.join(lines)
                            if len(content) > 10:
                                print(f"ğŸ«› ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ (ì„ íƒì: {sel}): {len(content)}ì")
                                break
                except Exception as e:
                    continue
            
            # ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (div.side.fr span b)
            view_cnt = 0
            like_cnt = 0
            comment_cnt = 0
            
            try:
                # div.side.fr ë‚´ë¶€ì˜ span ìš”ì†Œë“¤ ì°¾ê¸°
                side_div = await self.page.query_selector('div.side.fr')
                if side_div:
                    spans = await side_div.query_selector_all('span')
                    for span in spans:
                        span_text = await span.inner_text()
                        # "ì¡°íšŒ ìˆ˜" ë˜ëŠ” "ì¡°íšŒìˆ˜" íŒ¨í„´
                        if 'ì¡°íšŒ' in span_text:
                            b_tag = await span.query_selector('b')
                            if b_tag:
                                view_text = await b_tag.inner_text()
                                view_match = re.search(r'([\d,]+)', view_text)
                                if view_match:
                                    view_cnt = int(view_match.group(1).replace(',', ''))
                                    print(f"ğŸ«› ì¡°íšŒìˆ˜ ì¶”ì¶œ ì„±ê³µ: {view_text} -> {view_cnt}")
                        # "ì¶”ì²œ ìˆ˜" ë˜ëŠ” "ì¶”ì²œìˆ˜" íŒ¨í„´
                        elif 'ì¶”ì²œ' in span_text:
                            b_tag = await span.query_selector('b')
                            if b_tag:
                                like_text = await b_tag.inner_text()
                                like_match = re.search(r'([\d,]+)', like_text)
                                if like_match:
                                    like_cnt = int(like_match.group(1).replace(',', ''))
                                    print(f"ğŸ«› ì¶”ì²œìˆ˜ ì¶”ì¶œ ì„±ê³µ: {like_text} -> {like_cnt}")
                        # "ëŒ“ê¸€" íŒ¨í„´
                        elif 'ëŒ“ê¸€' in span_text:
                            b_tag = await span.query_selector('b')
                            if b_tag:
                                comment_text = await b_tag.inner_text()
                                comment_match = re.search(r'([\d,]+)', comment_text)
                                if comment_match:
                                    comment_cnt = int(comment_match.group(1).replace(',', ''))
                                    print(f"ğŸ«› ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ ì„±ê³µ: {comment_text} -> {comment_cnt}")
            except Exception as e:
                print(f"ğŸ«› í†µê³„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # ì‘ì„±ì¼ì‹œ ì¶”ì¶œ (span.date.m_no)
            created_at = None
            try:
                date_elem = await self.page.query_selector('span.date.m_no, .date.m_no')
                if date_elem:
                    date_text = await date_elem.inner_text()
                    if date_text:
                        created_at = self._parse_date(date_text)
                        if created_at:
                            print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì„±ê³µ: {date_text} -> {created_at}")
                
                # ëŒ€ì²´ ë°©ë²•: div.top_area ë‚´ë¶€ì—ì„œ ì°¾ê¸°
                if not created_at:
                    top_area = await self.page.query_selector('div.top_area')
                    if top_area:
                        date_elem = await top_area.query_selector('span.date, .date')
                        if date_elem:
                            date_text = await date_elem.inner_text()
                            if date_text:
                                created_at = self._parse_date(date_text)
                                if created_at:
                                    print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì„±ê³µ (top_area): {date_text} -> {created_at}")
            except Exception as e:
                print(f"ğŸ«› ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # URL ì¶”ì¶œ (div.document_address a ë˜ëŠ” data-clipboard-text)
            actual_url = post_url
            try:
                # ë°©ë²• 1: div.document_address a íƒœê·¸
                doc_address = await self.page.query_selector('div.document_address')
                if doc_address:
                    a_tag = await doc_address.query_selector('a')
                    if a_tag:
                        url_text = await a_tag.inner_text()
                        if url_text and 'fmkorea.com' in url_text:
                            actual_url = url_text.strip()
                            print(f"ğŸ«› URL ì¶”ì¶œ ì„±ê³µ (document_address): {actual_url}")
                
                # ë°©ë²• 2: data-clipboard-text ì†ì„±
                if actual_url == post_url:
                    copy_button = await self.page.query_selector('button[data-clipboard-text]')
                    if copy_button:
                        clipboard_url = await copy_button.get_attribute('data-clipboard-text')
                        if clipboard_url and 'fmkorea.com' in clipboard_url:
                            actual_url = clipboard_url
                            print(f"ğŸ«› URL ì¶”ì¶œ ì„±ê³µ (clipboard): {actual_url}")
            except Exception as e:
                print(f"ğŸ«› URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            print(f"ğŸ«› ì¶”ì¶œ ì™„ë£Œ: title={title[:30]}..., view_cnt={view_cnt}, comment_cnt={comment_cnt}, like_cnt={like_cnt}, own_company={own_company}")
            
            return Post(
                id=None,
                channel=self.channel,
                category="",
                title=title.strip() if title else "",
                content=content.strip() if content else "",
                view_cnt=view_cnt,
                like_cnt=like_cnt,
                comment_cnt=comment_cnt,
                created_at=created_at,
                own_company=own_company,
                url=actual_url
            )
                
        except Exception as e:
            print(f"ğŸ«› ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
