from src.crawlers.base_crawler import BaseCrawler
from src.models.post import Post
from typing import List, Dict, Optional
import re
from datetime import datetime, timedelta


class PpomppuCrawler(BaseCrawler):
    def __init__(self):
        super().__init__()
        self.popular_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu&hotlist_flag=999"
        self.channel = "ppomppu"
    
    async def crawl(self, max_posts: int = None) -> List[Post]:
        """ë½ë¿Œ ì¸ê¸°ê¸€ í¬ë¡¤ë§ (ì˜¤ëŠ˜ ê¸°ì¤€ ì¼ì£¼ì¼ ì „ê¹Œì§€)"""
        posts = []
        
        try:
            # 1. ì¸ê¸°ê¸€ í˜ì´ì§€ ì ‘ì† (ë¡œê·¸ì¸ ë¶ˆí•„ìš”)
            print(f"ğŸ«› ì¸ê¸°ê¸€ í˜ì´ì§€ ì ‘ì†: {self.popular_url}")
            
            # ì¬ì‹œë„ ë¡œì§
            max_retries = 3
            for retry in range(max_retries):
                try:
                    # í˜ì´ì§€ ì ‘ì†
                    response = await self.page.goto(
                        self.popular_url, 
                        wait_until="load",
                        timeout=30000
                    )
                    if response:
                        print(f"ğŸ«› í˜ì´ì§€ ì‘ë‹µ ìƒíƒœ: {response.status}")
                        if response.status != 200:
                            raise Exception(f"HTTP ìƒíƒœ ì½”ë“œ ì˜¤ë¥˜: {response.status}")
            
                    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ê³ ë ¤)
                    await self.page.wait_for_timeout(3000)
            
                    # í˜ì´ì§€ ë‚´ìš© í™•ì¸
                    page_title = await self.page.title()
                    print(f"ğŸ«› í˜ì´ì§€ ì œëª©: {page_title[:50]}...")
                    
                    # ì‹¤ì œ HTML êµ¬ì¡° í™•ì¸
                    page_content = await self.page.content()
                    print(f"ğŸ«› í˜ì´ì§€ ê¸¸ì´: {len(page_content)} bytes")
                    
                    # ë‹¤ì–‘í•œ ì„ íƒìë¡œ í…Œì´ë¸” ì°¾ê¸° ì‹œë„
                    selectors_to_try = [
                        '.list_table',
                        'table.list_table',
                        '.board_table',
                        'table.board_table',
                        'table',
                        '[class*="list"]',
                        '[class*="table"]',
                        '[id*="list"]'
                    ]
                    
                    table_found = False
                    for selector in selectors_to_try:
                        try:
                            element = await self.page.query_selector(selector)
                            if element:
                                print(f"ğŸ«› í…Œì´ë¸” ë°œê²¬: {selector}")
                                # ê²Œì‹œê¸€ í–‰ í™•ì¸
                                row_count = await self.page.evaluate(f"document.querySelectorAll('{selector} tr').length")
                                print(f"ğŸ«› ë°œê²¬ëœ ê²Œì‹œê¸€ í–‰ ìˆ˜: {row_count}")
                                if row_count > 0:
                                    table_found = True
                                    break
                        except:
                            continue
                    
                    if not table_found:
                        # í˜ì´ì§€ HTML ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…)
                        body_text = await self.page.evaluate("document.body.innerText")
                        print(f"ğŸ«› í˜ì´ì§€ ë³¸ë¬¸ ì¼ë¶€: {body_text[:200]}...")
                        raise Exception(f"ê²Œì‹œê¸€ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡° í™•ì¸ í•„ìš”.")
                    
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        print(f"ğŸ«› í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retry + 1}/{max_retries}): {e}")
                        await self.page.wait_for_timeout(5000)  # ì¬ì‹œë„ ì „ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                    else:
                        print(f"ğŸ«› í˜ì´ì§€ ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
            
            # 2. ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ (ì¼ì£¼ì¼ ì „ê¹Œì§€ í•„í„°ë§, ë²ˆí˜¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸)
            post_items = await self._get_posts_from_popular_page(max_posts)
            print(f"ğŸ«› ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ëª©ë¡: {len(post_items)}ê°œ")
            
            # 3. ê° ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            for i, item in enumerate(post_items):
                try:
                    post_url = item.get('url', '')
                    comment_cnt = item.get('comment_cnt', 0)
                    title = item.get('title', '')
                    
                    print(f"ğŸ«› ê²Œì‹œê¸€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {post_url} [{i+1}/{len(post_items)}]")
                    
                    # ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì ‘ì† (ì •ì  í˜ì´ì§€)
                    await self.page.goto(
                        post_url, 
                        wait_until="load",
                        timeout=30000
                    )
                    await self.page.wait_for_timeout(1000)
                        
                    # ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ
                    post = await self._extract_post_data(post_url, comment_cnt, title)
                    if post:
                        posts.append(post)
                        
                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                        await self.page.wait_for_timeout(1000)
                        
                except Exception as e:
                    print(f"ğŸ«› ê²Œì‹œê¸€ {post_url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                    
        except Exception as e:
            print(f"ğŸ«› ë½ë¿Œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"ğŸ«› ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts
    
    async def _get_posts_from_popular_page(self, max_posts: int = None) -> List[Dict]:
        """ì¸ê¸°ê¸€ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ë¥¼ ìˆ˜ì§‘ (ì˜¤ëŠ˜ ê¸°ì¤€ ì¼ì£¼ì¼ ì „ê¹Œì§€, ë²ˆí˜¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸)"""
        print(f"ğŸ«› ì¸ê¸°ê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        
        # ë‚ ì§œ í•„í„°: ì˜¤ëŠ˜ ê¸°ì¤€ ì¼ì£¼ì¼ ì „
        today = datetime.now()
        week_ago = today - timedelta(days=7)
        print(f"ğŸ«› ë‚ ì§œ í•„í„°: {week_ago.strftime('%Y-%m-%d')} ~ {today.strftime('%Y-%m-%d')}")
        
        collected_items: List[Dict] = []
        current_page = 1
        max_pages = 200  # ì¶©ë¶„íˆ í° ê°’ (ì¼ì£¼ì¼ ì „ê¹Œì§€ ëª¨ë“  í˜ì´ì§€ë¥¼ íƒìƒ‰)
        
        while current_page <= max_pages:
            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            await self.page.wait_for_timeout(2000)
            
            items = await self.page.evaluate("""
                (() => {
                    const items = [];
                    // ë‹¤ì–‘í•œ í…Œì´ë¸” ì„ íƒì ì‹œë„
                    let rows = [];
                    const selectors = [
                        '.list_table tr',
                        'table.list_table tr',
                        '.board_table tr',
                        'table.board_table tr',
                        'table tr',
                        '[class*="list"] tr',
                        '[class*="table"] tr'
                    ];
                    
                    for (const sel of selectors) {
                        rows = Array.from(document.querySelectorAll(sel));
                        if (rows.length > 0) {
                            console.log('í…Œì´ë¸” ë°œê²¬:', sel, 'í–‰ ìˆ˜:', rows.length);
                            break;
                        }
                    }
                    
                    for (const row of rows) {
                        // ë²ˆí˜¸ ì»¬ëŸ¼ ì°¾ê¸° (ì²« ë²ˆì§¸ td)
                        const noTd = row.querySelector('td:first-child');
                        if (!noTd) continue;
                        
                        const noText = noTd.innerText.trim();
                        // ë²ˆí˜¸ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ê³µë°±, "-", "ê³µì§€" ë“± ì œì™¸)
                        if (!noText || noText === '-' || noText === 'ê³µì§€' || isNaN(parseInt(noText))) {
                            continue;
                        }
                        
                        // ì œëª© ë§í¬ ì°¾ê¸°
                        const titleLink = row.querySelector('td.title a, a[href*="view.php"]');
                        if (!titleLink) continue;
                        
                        const href = titleLink.getAttribute('href');
                        if (!href) continue;
                        
                        // URL ìƒì„±
                        let fullUrl = href;
                        if (href.startsWith('/')) {
                            fullUrl = 'https://www.ppomppu.co.kr' + href;
                        } else if (href.startsWith('view.php')) {
                            fullUrl = 'https://www.ppomppu.co.kr/zboard/' + href;
                        } else if (!href.startsWith('http')) {
                            fullUrl = 'https://www.ppomppu.co.kr/zboard/' + href;
                        }
                        
                        // ì œëª© ì¶”ì¶œ
                        const titleText = titleLink.innerText.trim() || titleLink.textContent.trim();
                        
                        // ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì œëª© ë§ˆì§€ë§‰ ë¶€ë¶„ ë˜ëŠ” span.baseList-c)
                        let commentCount = 0;
                        const commentSpan = row.querySelector('span.baseList-c');
                        if (commentSpan) {
                            const commentText = commentSpan.innerText.trim();
                            const match = commentText.match(/(\\d+)/);
                            if (match) {
                                commentCount = parseInt(match[1]);
                            }
                        }
                        
                        // ì œëª©ì—ì„œë„ ëŒ“ê¸€ìˆ˜ ì°¾ê¸° (ì œëª© ë’¤ ìˆ«ì íŒ¨í„´)
                        if (commentCount === 0) {
                            // ì œëª© ë§ˆì§€ë§‰ ìˆ«ì ì°¾ê¸° (ì˜ˆ: "...7 [ê°€ì „/ì „ì]")
                            const titleMatch = titleText.match(/(\\d+)\\s*\\[[^\\]]+\\]$/);
                            if (titleMatch) {
                                commentCount = parseInt(titleMatch[1]);
                            }
                        }
                        
                        // ë‚ ì§œ ì¶”ì¶œ (ì¼ë°˜ì ìœ¼ë¡œ ë‚ ì§œ ì»¬ëŸ¼)
                        let dateText = '';
                        const dateCells = row.querySelectorAll('td');
                        for (const cell of dateCells) {
                            const text = cell.innerText.trim();
                            // ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YY/MM/DD ë˜ëŠ” HH:MM:SS)
                            if (text.match(/\\d{2}\\/\\d{2}\\/\\d{2}/) || text.match(/\\d{2}:\\d{2}:\\d{2}/)) {
                                dateText = text;
                                break;
                            }
                        }
                        
                        // ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì œëª©ì—ì„œ [ì¹´í…Œê³ ë¦¬] íŒ¨í„´)
                        let category = '';
                        const categoryMatch = titleText.match(/^\\[([^\\]]+)\\]/);
                        if (categoryMatch) {
                            category = categoryMatch[1];
                        }
                        
                        items.push({
                            url: fullUrl,
                            title: titleText,
                            dateText: dateText,
                            comment_cnt: commentCount,
                            category: category,
                            no: noText
                        });
                    }
                    
                    return items;
                })()
            """)
            
            print(f"ğŸ«› [í˜ì´ì§€ {current_page}] í™”ë©´ì—ì„œ ë°œê²¬ëœ ê²Œì‹œê¸€ ìˆ˜: {len(items)}")
            
            before_len = len(collected_items)
            found_old_posts = False  # ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ì´ ìˆëŠ”ì§€ í™•ì¸
            
            for item in items:
                url = item.get('url', '')
                title = item.get('title', '')
                date_text = item.get('dateText', '')
                
                # ë‚ ì§œ í•„í„°ë§ (ì¼ì£¼ì¼ ì „ê¹Œì§€)
                if date_text:
                    post_date = self._parse_date(date_text)
                    if post_date:
                        if post_date < week_ago:
                            print(f"ğŸ«› ì œì™¸: ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ - {date_text}")
                            found_old_posts = True  # ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ì´ ë°œê²¬ë¨
                            continue
                    else:
                        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì œì™¸ (ëª…í™•í•œ ë‚ ì§œê°€ í•„ìš”)
                        print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, ì œì™¸: {date_text}")
                        continue
                else:
                    # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì œì™¸
                    print(f"ğŸ«› ë‚ ì§œ ì •ë³´ ì—†ìŒ, ì œì™¸")
                    continue
                
                # ì¤‘ë³µ ì²´í¬ (URL ê¸°ë°˜)
                if url and url not in [item['url'] for item in collected_items]:
                    collected_items.append(item)
            
            after_len = len(collected_items)
            new_count = after_len - before_len
            print(f"ğŸ«› [í˜ì´ì§€ {current_page}] ì‹ ê·œ ìˆ˜ì§‘: {new_count}ê°œ, ëˆ„ì : {after_len}ê°œ")
            
            # ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ë§Œ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
            if found_old_posts and new_count == 0:
                print(f"ğŸ«› ì¼ì£¼ì¼ ì´ì „ ê²Œì‹œê¸€ë§Œ ë‚¨ì•„ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            # max_posts ì œí•œì´ ìˆìœ¼ë©´ ì²´í¬ (ë””ë²„ê¹…ìš©)
            if max_posts and len(collected_items) >= max_posts:
                print(f"ğŸ«› max_posts({max_posts})ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            if current_page < max_pages:
                next_page_clicked = False
                try:
                    # ë°©ë²• 1: ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ ì°¾ê¸° (div.bottom-list a.num)
                    next_page_num = current_page + 1
                    
                    # div.bottom-list ë‚´ë¶€ì˜ í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ ì°¾ê¸°
                    next_page_button = await self.page.query_selector(f'div.bottom-list a.num:has-text("{next_page_num}")')
                    if not next_page_button:
                        # hrefë¡œ ì°¾ê¸°
                        next_page_button = await self.page.query_selector(f'div.bottom-list a.num[href*="page={next_page_num}"]')
                    if not next_page_button:
                        # ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
                        next_page_button = await self.page.query_selector(f'a.num:has-text("{next_page_num}")')
                    
                    if next_page_button:
                        await next_page_button.click()
                        next_page_clicked = True
                        print(f"ğŸ«› í˜ì´ì§€ {next_page_num} ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                    
                    # ë°©ë²• 2: class="next" ë²„íŠ¼ í´ë¦­ (10í˜ì´ì§€ ì´í›„)
                    if not next_page_clicked:
                        next_button = await self.page.query_selector('div.bottom-list a.next, a.next')
                        if next_button:
                            # ë¹„í™œì„±í™” í™•ì¸
                            is_disabled = await next_button.get_attribute('disabled')
                            if not is_disabled:
                                await next_button.click()
                                next_page_clicked = True
                                print(f"ğŸ«› ë‹¤ìŒ ë²„íŠ¼(next) í´ë¦­ ì„±ê³µ - í˜ì´ì§€ {next_page_num} í‘œì‹œ ì˜ˆì •")
                    
                    if next_page_clicked:
                        await self.page.wait_for_timeout(3000)
                        # í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        current_url = self.page.url
                        if 'page=' in current_url:
                            page_match = re.search(r'page=(\d+)', current_url)
                            if page_match:
                                current_page = int(page_match.group(1))
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (URL í™•ì¸)")
                            else:
                                # URLì— í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ í™œì„± í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸
                                active_page = await self.page.evaluate("""
                                    (() => {
                                        const activeLink = document.querySelector('div.bottom-list a.num.on');
                                        if (activeLink) {
                                            return parseInt(activeLink.innerText.trim());
                                        }
                                        return null;
                                    })()
                                """)
                                if active_page:
                                    current_page = active_page
                                    print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (í™œì„± í˜ì´ì§€ í™•ì¸)")
                                else:
                                    current_page += 1
                                    print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (ì¶”ì •)")
                        else:
                            # URLì— í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ í™œì„± í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸
                            active_page = await self.page.evaluate("""
                                (() => {
                                    const activeLink = document.querySelector('div.bottom-list a.num.on');
                                    if (activeLink) {
                                        return parseInt(activeLink.innerText.trim());
                                    }
                                    return null;
                                })()
                            """)
                            if active_page:
                                current_page = active_page
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (í™œì„± í˜ì´ì§€ í™•ì¸)")
                            else:
                                current_page += 1
                                print(f"ğŸ«› í˜ì´ì§€ {current_page} ë¡œë“œ ì™„ë£Œ (ì¶”ì •)")
                    else:
                        print(f"ğŸ«›âŒ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í•¨. ìˆ˜ì§‘ ì¢…ë£Œ")
                        break
                        
                except Exception as e:
                    print(f"ğŸ«›âŒ í˜ì´ì§€ë„¤ì´ì…˜ í´ë¦­ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
                    break
            else:
                print(f"ğŸ«› ìµœëŒ€ í˜ì´ì§€({max_pages})ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
        
        # max_posts ì œí•œ ì ìš©
        if max_posts:
            post_items = collected_items[:max_posts]
        else:
            post_items = collected_items
        
        print(f"ğŸ«› ì´ {len(post_items)}ê°œ ì¸ê¸°ê¸€ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {current_page}í˜ì´ì§€ ìˆœíšŒ)")
        return post_items
    
    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (ëª©ë¡ í˜ì´ì§€ìš©)"""
        if not date_text:
            return None
        
        try:
            # í˜•ì‹ 1: "23:08:03" (ì˜¤ëŠ˜ ë‚ ì§œ - ì‹œê°„ë§Œ í‘œì‹œ)
            if re.match(r'^\d{2}:\d{2}:\d{2}$', date_text.strip()):
                today = datetime.now()
                time_parts = date_text.strip().split(':')
                return today.replace(hour=int(time_parts[0]), minute=int(time_parts[1]), second=int(time_parts[2]), microsecond=0)
            
            # í˜•ì‹ 2: "25/11/02" (YY/MM/DD)
            if re.match(r'^\d{2}/\d{2}/\d{2}$', date_text.strip()):
                parts = date_text.strip().split('/')
                year = 2000 + int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)
            
            # í˜•ì‹ 3: "2025-11-02 09:33" (YYYY-MM-DD HH:MM) - ìƒì„¸ í˜ì´ì§€ í˜•ì‹ë„ ì§€ì›
            match = re.search(r'(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2})', date_text)
            if match:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                hour = int(match.group(4))
                minute = int(match.group(5))
                return datetime(year, month, day, hour, minute)
            
            # í˜•ì‹ 4: "2025.11.02. 12:39" (YYYY.MM.DD. HH:MM)
            match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})\.?\s*(\d{1,2}):(\d{1,2})', date_text)
            if match:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                hour = int(match.group(4))
                minute = int(match.group(5))
                return datetime(year, month, day, hour, minute)
            
        except Exception as e:
            print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_text} - {e}")
            return None
        
        return None
    
    async def _extract_post_data(self, post_url: str, comment_cnt: int, title_from_list: str) -> Optional[Post]:
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ (h1 íƒœê·¸ì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ)
            title = title_from_list
            # h1 íƒœê·¸ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸°
            title_elem = await self.page.query_selector('h1')
            if not title_elem:
                # h1ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì„ íƒì ì‹œë„
                title_elem = await self.page.query_selector('span.topTitle, .topTitle, [class*="title"]')
            
            if title_elem:
                # h1 íƒœê·¸ì¸ ê²½ìš°: ì´ë¯¸ì§€, ì¹´í…Œê³ ë¦¬ span, ëŒ“ê¸€ ìˆ˜ span ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                if await title_elem.evaluate('el => el.tagName.toLowerCase() === "h1"'):
                    title_text = await title_elem.evaluate("""
                        (elem) => {
                            // ì´ë¯¸ì§€, ì¹´í…Œê³ ë¦¬ span, ëŒ“ê¸€ ìˆ˜ span ì œê±°
                            const clone = elem.cloneNode(true);
                            clone.querySelectorAll('img, span#comment, span[id*="comment"], span.subject_preface, span[class*="preface"], span[class*="subject"]').forEach(el => el.remove());
                            // í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì‹¤ì œ ì œëª© í…ìŠ¤íŠ¸ë§Œ)
                            return clone.innerText.trim();
                        }
                    """)
                    if title_text and title_text.strip():
                        title = title_text.strip()
                else:
                    # h1ì´ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    title_text = await title_elem.inner_text()
                    if title_text and title_text.strip():
                        title = title_text.strip()
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì›ë³¸ ì œëª©ì—ì„œ, h1ì—ì„œ ì¶”ì¶œí•œ ê²½ìš° ì¹´í…Œê³ ë¦¬ spanì—ì„œ ì§ì ‘ ì¶”ì¶œ)
            category = ''
            if title_elem and await title_elem.evaluate('el => el.tagName.toLowerCase() === "h1"'):
                # h1ì—ì„œ ì¹´í…Œê³ ë¦¬ span ì§ì ‘ ì¶”ì¶œ
                category_text = await title_elem.evaluate("""
                    (elem) => {
                        const categorySpan = elem.querySelector('span.subject_preface, span[class*="preface"], span[class*="subject"]');
                        if (categorySpan) {
                            const text = categorySpan.innerText.trim();
                            // [ë„¤ì´ë²„] í˜•ì‹ì—ì„œ ë„¤ì´ë²„ë§Œ ì¶”ì¶œ
                            const match = text.match(/\\[([^\\]]+)\\]/);
                            return match ? match[1] : text;
                        }
                        return '';
                    }
                """)
                if category_text:
                    category = category_text
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (fallback: ì œëª©ì—ì„œ íŒ¨í„´ ë§¤ì¹­)
            if not category:
                category_match = re.search(r'^\[([^\]]+)\]', title)
                if category_match:
                    category = category_match.group(1)
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ë§Œ, UI ìš”ì†Œ ì œì™¸)
            content = ""
            content_selectors = [
                'table.board-contents',  # ë” ì •í™•í•œ ì„ íƒì
                '.board-contents table',
                '.board-contents',
                '[class*="contents"]:not([class*="menu"]):not([class*="nav"])',
                '.view_content',
                '#article'
            ]
            
            # UI ê´€ë ¨ í…ìŠ¤íŠ¸ ëª©ë¡ (ë³¸ë¬¸ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ íŒë‹¨)
            # ì£¼ì˜: ì‹¤ì œ ë³¸ë¬¸ì—ë„ í¬í•¨ë  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸
            ui_keywords = [
                'ë½ë¿Œ', 'ì´ë²¤íŠ¸', 'ì •ë³´', 'ì»¤ë®¤ë‹ˆí‹°', 'ê°¤ëŸ¬ë¦¬', 'ì¥í„°', 'í¬ëŸ¼', 'ë‰´ìŠ¤', 'ìƒë‹´ì‹¤',
                'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ì•„ì´ë””ë¹„ë²ˆì°¾ê¸°', 'ë½ë¿Œê²Œì‹œíŒ', 'ì‚¬ìš©ê¸°', 'êµ¬ë§¤í›„ê¸°',
                'ì¿ í°ê²Œì‹œíŒ', 'ì‡¼í•‘í¬ëŸ¼', 'ë½ë¿Œí•«ë”œ', 'ëª©ë¡ë³´ê¸°', 'ìµœì‹ ìˆœ', 'ì‘ì„±ìˆœ',
                'ì•Œë¦¼', 'ê´‘ê³ ì„± ê²Œì‹œê¸€', 'ì—ë””í„°', 'HTMLí¸ì§‘', 'ë¯¸ë¦¬ë³´ê¸°', 'ì§¤ë°©',
                'ì—…ìì‹ ê³ ', 'ë‹¤ë¥¸ì˜ê²¬', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€', 'ë“±ë¡ì¼', 'ì¡°íšŒìˆ˜', 'ì¶”ì²œí•˜ê¸°',
                'ì§ˆë €ì–´ìš” ì‹ ê³ ', 'ì²¨ë¶€íŒŒì¼', 'ê°™ì´ ë³´ë©´ ì¢‹ì€ ìƒí’ˆ',  # 'ìƒí’ˆ' ë‹¨ë… ì œê±°, 'ê°™ì´ ë³´ë©´ ì¢‹ì€ ìƒí’ˆ'ë§Œ
                'êµ¬ë§¤í•˜ì…¨ë‹¤ë©´', 'í›„ê¸°ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”', 'êµ¬ë§¤í›„ê¸° ì“°ê¸°'
            ]
            
            for sel in content_selectors:
                try:
                    content_elem = await self.page.query_selector(sel)
                    if content_elem:
                        # UI ìš”ì†Œ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        content_text = await content_elem.evaluate("""
                            (elem) => {
                                const clone = elem.cloneNode(true);
                                
                                // ë©”ë‰´, ë„¤ë¹„ê²Œì´ì…˜, ì‚¬ì´ë“œë°” ì œê±°
                                clone.querySelectorAll('[class*="menu"], [class*="nav"], [class*="sidebar"], [id*="menu"], [id*="nav"], [id*="sidebar"]').forEach(el => el.remove());
                                
                                // ëŒ“ê¸€ ì˜ì—­ ì œê±°
                                clone.querySelectorAll('[class*="comment"], [class*="reply"], [id*="comment"], [id*="reply"], [class*="reply_box"], [class*="comment_box"]').forEach(el => el.remove());
                                
                                // ì¶”ì²œí•˜ê¸°, ë‹¤ë¥¸ì˜ê²¬, ì§ˆë €ì–´ìš” ì‹ ê³ , ì²¨ë¶€íŒŒì¼, ì´ì „ê¸€/ë‹¤ìŒê¸€ ë²„íŠ¼ ì œê±°
                                clone.querySelectorAll('[class*="recommend"], [class*="like"], [class*="attach"], [class*="prev"], [class*="next"], [class*="list"]').forEach(el => el.remove());
                                
                                // "ê°™ì´ ë³´ë©´ ì¢‹ì€ ìƒí’ˆ" ì˜ì—­ ì œê±° (ì •í™•í•œ ë¬¸êµ¬ë¡œë§Œ)
                                const relatedProducts = Array.from(clone.querySelectorAll('*')).filter(el => {
                                    const text = el.innerText || el.textContent || '';
                                    // ì •í™•íˆ "ê°™ì´ ë³´ë©´ ì¢‹ì€ ìƒí’ˆ" ë˜ëŠ” ìœ ì‚¬í•œ íŒ¨í„´ë§Œ ì œê±°
                                    return text.includes('ê°™ì´ ë³´ë©´ ì¢‹ì€') && (text.includes('ìƒí’ˆ') || text.includes('ì¶”ì²œ'));
                                });
                                relatedProducts.forEach(el => el.remove());
                                
                                // "êµ¬ë§¤í•˜ì…¨ë‹¤ë©´" ê°™ì€ ì•ˆë‚´ ë¬¸êµ¬ ì œê±°
                                const guideTexts = Array.from(clone.querySelectorAll('*')).filter(el => {
                                    const text = el.innerText || el.textContent || '';
                                    return text.includes('êµ¬ë§¤í•˜ì…¨ë‹¤ë©´') || text.includes('í›„ê¸°ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”') || text.includes('êµ¬ë§¤í›„ê¸° ì“°ê¸°');
                                });
                                guideTexts.forEach(el => el.remove());
                                
                                // ì´ë¯¸ì§€ ì œê±°
                                clone.querySelectorAll('img').forEach(el => el.remove());
                                
                                // ë§í¬ëŠ” ì œê±°í•˜ë˜ í…ìŠ¤íŠ¸ëŠ” ìœ ì§€ (URLì€ ì œê±°)
                                clone.querySelectorAll('a').forEach(link => {
                                    const href = link.getAttribute('href') || '';
                                    // URL ë§í¬ëŠ” ì œê±°
                                    if (href.startsWith('http') || href.startsWith('//')) {
                                        link.remove();
                                    } else {
                                        // ìƒëŒ€ ë§í¬ëŠ” í…ìŠ¤íŠ¸ë§Œ ìœ ì§€
                                        const textNode = document.createTextNode(link.innerText || link.textContent || '');
                                        link.parentNode.replaceChild(textNode, link);
                                    }
                                });
                                
                                return clone.innerText || clone.textContent || '';
                            }
                        """)
                        
                        if content_text:
                            # ì¤„ë°”ê¿ˆ ì •ë¦¬ (ë¹ˆ ì¤„ ì œê±°)
                            lines = [line.strip() for line in content_text.split('\n') if line.strip()]
                            
                            # ë©”íƒ€ ì •ë³´ ë¼ì¸ ì œê±° (ë“±ë¡ì¼, ì¡°íšŒìˆ˜, ì¶”ì²œ ë“±)
                            filtered_lines = []
                            for line in lines:
                                # ë©”íƒ€ ì •ë³´ íŒ¨í„´ ì œì™¸
                                if (re.match(r'^(ë“±ë¡ì¼|ì¡°íšŒìˆ˜|ì¶”ì²œ)\s*\d+', line) or
                                    re.match(r'^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}', line) or  # ë‚ ì§œ íŒ¨í„´
                                    re.match(r'^https?://', line) or  # URL
                                    re.match(r'^\d+ì›$', line) or  # ê°€ê²©ë§Œ ìˆëŠ” ë¼ì¸
                                    line in ['ë“±ë¡ì¼', 'ì¡°íšŒìˆ˜', 'ì¶”ì²œ', 'ì¶”ì²œí•˜ê¸°', 'ë‹¤ë¥¸ì˜ê²¬', 'ì§ˆë €ì–´ìš” ì‹ ê³ ']):
                                    continue
                                filtered_lines.append(line)
                            
                            content = '\n'.join(filtered_lines)
                            
                            # UI í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì •í™•í•œ ë§¤ì¹­)
                            # ê° ë¼ì¸ì—ì„œ UI í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            ui_keyword_lines = 0
                            for line in filtered_lines:
                                for keyword in ui_keywords:
                                    if keyword in line:
                                        ui_keyword_lines += 1
                                        break  # í•œ ë¼ì¸ì—ì„œ í•˜ë‚˜ì˜ í‚¤ì›Œë“œë§Œ ì¹´ìš´íŠ¸
                            
                            total_lines = len(filtered_lines)
                            ui_ratio = ui_keyword_lines / max(total_lines, 1) if total_lines > 0 else 0
                            
                            # ì‹¤ì œ ë³¸ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸ (ì¼ì • ê¸¸ì´ ì´ìƒì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€)
                            has_meaningful_content = False
                            for line in filtered_lines:
                                # UI í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë¼ì¸ ì¤‘ ê¸¸ì´ê°€ ì¶©ë¶„í•œ ë¼ì¸ì´ ìˆëŠ”ì§€
                                is_ui_line = any(keyword in line for keyword in ui_keywords)
                                if not is_ui_line and len(line) > 20:  # 20ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë¼ì¸
                                    has_meaningful_content = True
                                    break
                            
                            # UI í‚¤ì›Œë“œ ë¹„ìœ¨ì´ ë†’ê³ (50% ì´ìƒ) ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ë³¸ë¬¸ ì—†ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨
                            # ë˜ëŠ” UI í‚¤ì›Œë“œ ë¼ì¸ì´ 10ê°œ ì´ìƒì´ë©´ ë³¸ë¬¸ ì—†ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨
                            if (ui_ratio >= 0.5 and not has_meaningful_content) or ui_keyword_lines >= 10:
                                print(f"ğŸ«› UI ìš”ì†Œê°€ ë§ì´ í¬í•¨ë˜ì–´ ë³¸ë¬¸ ì—†ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨ (UI í‚¤ì›Œë“œ ë¼ì¸: {ui_keyword_lines}ê°œ, ë¹„ìœ¨: {ui_ratio:.2f}, ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸: {has_meaningful_content})")
                                content = ""  # ë³¸ë¬¸ì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                                continue  # ë‹¤ìŒ ì„ íƒì ì‹œë„
                            
                            if len(content) > 10:
                                print(f"ğŸ«› ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ (ì„ íƒì: {sel}): {len(content)}ì")
                                break
                except Exception as e:
                    continue
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ ("ì¡°íšŒìˆ˜" í…ìŠ¤íŠ¸ ì´í›„ ê°’)
            view_cnt = 0
            try:
                page_text = await self.page.evaluate("document.body.innerText")
                view_match = re.search(r'ì¡°íšŒìˆ˜\s*[:ï¼š]?\s*([\d,]+)', page_text)
                if view_match:
                    view_num_str = view_match.group(1).replace(',', '')
                    view_cnt = int(view_num_str)
                    print(f"ğŸ«› ì¡°íšŒìˆ˜ ì¶”ì¶œ ì„±ê³µ: {view_match.group(1)} -> {view_cnt}")
            except Exception as e:
                pass
            
            # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ (span.topTitle-rec em íƒœê·¸ ë‚´ ìˆ«ì)
            like_cnt = 0
            try:
                rec_span = await self.page.query_selector('span.topTitle-rec')
                if rec_span:
                    em_tag = await rec_span.query_selector('em')
                    if em_tag:
                        like_text = await em_tag.inner_text()
                        like_match = re.search(r'(\d+)', like_text)
                        if like_match:
                            like_cnt = int(like_match.group(1))
                            print(f"ğŸ«› ì¶”ì²œìˆ˜ ì¶”ì¶œ ì„±ê³µ: {like_text} -> {like_cnt}")
            except Exception as e:
                pass
            
            # ì‘ì„±ì¼ì‹œ ì¶”ì¶œ ("ë“±ë¡ì¼" ì´í›„ ê°’) - ul.topTitle-mainbox li ìš”ì†Œì—ì„œ ì¶”ì¶œ
            created_at = None
            try:
                # ë°©ë²• 1: ul.topTitle-mainbox li ìš”ì†Œì—ì„œ "ë“±ë¡ì¼ YYYY-MM-DD HH:MM" í˜•ì‹ ì°¾ê¸°
                mainbox = await self.page.query_selector('ul.topTitle-mainbox')
                if mainbox:
                    li_elements = await mainbox.query_selector_all('li')
                    for li in li_elements:
                        li_text = await li.inner_text()
                        # "ë“±ë¡ì¼ 2025-11-02 09:33" í˜•ì‹ ì°¾ê¸°
                        date_match = re.search(r'ë“±ë¡ì¼\s+(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', li_text)
                        if date_match:
                            date_str = date_match.group(1).strip()
                            try:
                                created_at = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
                                print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì„±ê³µ (topTitle-mainbox): {date_str} -> {created_at}")
                                break
                            except:
                                continue
                
                # ë°©ë²• 2: í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ "ë“±ë¡ì¼ YYYY-MM-DD HH:MM" í˜•ì‹ ì°¾ê¸°
                if not created_at:
                    page_text = await self.page.evaluate("document.body.innerText")
                    date_match = re.search(r'ë“±ë¡ì¼\s+(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', page_text)
                    if date_match:
                        date_str = date_match.group(1).strip()
                        try:
                            created_at = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
                            print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì„±ê³µ (ì „ì²´ í…ìŠ¤íŠ¸): {date_str} -> {created_at}")
                        except:
                            pass
                
                # ë°©ë²• 3: ê¸°ì¡´ íŒ¨í„´ (YY/MM/DD ë˜ëŠ” HH:MM:SS) - í•˜ìœ„ í˜¸í™˜ì„±
                if not created_at:
                    page_text = await self.page.evaluate("document.body.innerText")
                    date_match = re.search(r'ë“±ë¡ì¼[^\n]*[:ï¼š]?\s*(\d{2}/\d{2}/\d{2}|\d{2}:\d{2}:\d{2})', page_text)
                    if date_match:
                        date_text = date_match.group(1).strip()
                        created_at = self._parse_date(date_text)
                        if created_at:
                            print(f"ğŸ«› ë‚ ì§œ íŒŒì‹± ì„±ê³µ (ê¸°ì¡´ íŒ¨í„´): {date_text} -> {created_at}")
            except Exception as e:
                print(f"ğŸ«› ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # URL ì¶”ì¶œ (span.topTitle-copy í´ë¦­í•˜ì—¬ í´ë¦½ë³´ë“œì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            actual_url = post_url
            try:
                copy_span = await self.page.query_selector('span.topTitle-copy')
                if copy_span:
                    await copy_span.click()
                    await self.page.wait_for_timeout(500)
                    # í´ë¦½ë³´ë“œì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
                    clipboard_text = await self.page.evaluate("navigator.clipboard.readText()")
                    if clipboard_text and ('ppomppu.co.kr' in clipboard_text or 'view.php' in clipboard_text):
                        actual_url = clipboard_text
                        print(f"ğŸ«› URL í´ë¦½ë³´ë“œ ë³µì‚¬ ì„±ê³µ: {actual_url}")
            except Exception as e:
                # í´ë¦½ë³´ë“œ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ì‚¬ìš©
                pass
            
            # ê²Œì‹œê¸€ ID ì¶”ì¶œ (URLì—ì„œ)
            article_id = None
            id_match = re.search(r'no=(\d+)', post_url)
            if id_match:
                article_id = int(id_match.group(1))
            
            # own_company: ì œëª©ì— "ë¡¯ë°ì˜¨"ì´ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0
            own_company = 1 if title and 'ë¡¯ë°ì˜¨' in title else 0
            
            # contentê°€ ì—†ê±°ë‚˜ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ì—†ìœ¼ë©´ None ë°˜í™˜ (pass)
            content_cleaned = content.strip() if content else ""
            if not content_cleaned or len(content_cleaned) < 10:
                print(f"ğŸ«› contentê°€ ì—†ì–´ì„œ ê²Œì‹œë¬¼ ì œì™¸: {post_url}")
                return None
            
            print(f"ğŸ«› ì¶”ì¶œ ì™„ë£Œ: title={title[:30]}..., view_cnt={view_cnt}, comment_cnt={comment_cnt}, like_cnt={like_cnt}")
            
            return Post(
                id=article_id,
                channel=self.channel,
                category="",  # categoryëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ê³ ì •
                title=title.strip() if title else "",
                content=content_cleaned,
                view_cnt=view_cnt,
                like_cnt=like_cnt,
                comment_cnt=comment_cnt,
                created_at=created_at,
                own_company=own_company,
                url=actual_url
            )
                
        except Exception as e:
            print(f"ğŸ«› ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
